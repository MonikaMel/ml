{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action üé¨\n",
    "\n",
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()\n",
    "    \n",
    "def linear(vector):\n",
    "    \n",
    "    \"\"\"\n",
    "    We need to calculate WTx + b \n",
    "    But our vector is ,indeed, the same\n",
    "    \n",
    "    Nevermind, you'll understand later \n",
    "    \n",
    "    \"\"\"\n",
    "    return vector\n",
    "\n",
    "\n",
    "class FirstNeuralNework:\n",
    "    # Hey, sailor! Welcome aboard, we are always glad to new friends.\n",
    "    # Always follow Captain's orders. \n",
    "    # Remember, our goal is to find a treasure map. üíé üó∫Ô∏è\n",
    "    \n",
    "    def __init__(self, layers_vector, outputs_type):\n",
    "        \n",
    "        if outputs_type.upper() == \"PROBABILITY\":\n",
    "            self.activation_way = softmax\n",
    "        else:\n",
    "            self.activation_way = linear   \n",
    "        \"\"\"\n",
    "        Some information about\n",
    "        \n",
    "        layers_vector -- [0] --> number of input neurons\n",
    "                      -- [1:-1]  --> lenght of each layer\n",
    "                      -- [-1] --> number of output neurons              \n",
    "        \n",
    "         Setting random weights \n",
    "         weights[-1] --> bias\n",
    "         \n",
    "         \"\"\"\n",
    "        \n",
    "        self.layers_vector = layers_vector\n",
    "        self.weights = []\n",
    "        for i in range(1, len(self.layers_vector) - 1):\n",
    "            # Setting weights at (-1:1)\n",
    "            w = 2 * np.random.random((self.layers_vector[i-1], self.layers_vector[i] + 1)) - 1\n",
    "            # Adding one here <self.layers_vector[i-1] + 1> cause of bias \n",
    "            \n",
    "            self.weights.append(w)\n",
    "            \n",
    "        w = 2 * np.random.random((self.layers_vector[i], layers_vector[i + 1] + 1)) -1\n",
    "        self.weights.append(w)                        \n",
    "        # Now we have the First clue to the pirate treasure map.üóù\n",
    "    \n",
    "    def summing(self, x, index):\n",
    "        print(self.weights[index])\n",
    "        return np.dot(x.T, self.weights[index])\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, addition):\n",
    "        print(addition)\n",
    "        return 1 / (1 + np.e ** (-addition))\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, x): \n",
    "        \n",
    "        \"\"\"\n",
    "        X-INPUT --> vector or matrix\n",
    "        \n",
    "        Now we are going to push forward our model\n",
    "        \n",
    "        Adding ones at the end for bias\n",
    "        If x is 1d np.hstack will return an error\n",
    "        \"\"\" \n",
    "        \n",
    "        try:\n",
    "            n = np.array(x).shape[0]\n",
    "            ones_column = np.ones((n,1))\n",
    "            x = np.hstack((x, ones_column))\n",
    "            print(x)\n",
    "        except ValueError:\n",
    "            x += [1]\n",
    "        \n",
    "        dont_forget = dict()\n",
    "        n = x.shape[0]\n",
    "        act = x\n",
    "        for index in range(len(self.layers_vector) - 1):\n",
    "            print(index)\n",
    "            preact = self.summing(act[:n], index)\n",
    "            act = self.sigmoid(preact)\n",
    "            dont_forget[\"Preactivation\" + str(index + 1)] = preact\n",
    "            dont_forget[\"Activation\" + str(index + 1)] = act\n",
    "         \n",
    "        preact = self.summing(np.transpose(act), index)\n",
    "        act = self.activation_way(preact)\n",
    "        dont_forget[\"Preactivation\" + str(index + 1)] = preact\n",
    "        dont_forget[\"Activation\" + str(index + 1)] = act\n",
    "        \n",
    "        return act, dont_forget\n",
    "    \n",
    "        # Set sail! There is a Shark.ü¶àüè¥‚Äç‚ò†Ô∏è\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    To be continued...\n",
    "\n",
    "    \"\"\"                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.15827445, -0.19490208, -0.58804668,  0.0919204 ],\n",
       "        [ 0.5864056 ,  0.69929738,  0.48443954,  0.12202542],\n",
       "        [-0.33655052,  0.77142713, -0.98224302,  0.66562357]]),\n",
       " array([[-0.22550899, -0.69893516,  0.44223119,  0.4635144 ],\n",
       "        [ 0.41924637, -0.84907101,  0.44201946,  0.81895096],\n",
       "        [-0.31819419, -0.93157191, -0.88291063,  0.96730709]]),\n",
       " array([[-0.18535594, -0.28874483, -0.5699457 ],\n",
       "        [ 0.73566822, -0.41395986,  0.71734574],\n",
       "        [ 0.92613781, -0.03210042,  0.79083046]])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay = [3,3,3,2]\n",
    "test = FirstNeuralNework(lay, \"Probability\")\n",
    "test.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 0.15827445 -0.19490208 -0.58804668  0.0919204 ]\n",
      " [ 0.5864056   0.69929738  0.48443954  0.12202542]\n",
      " [-0.33655052  0.77142713 -0.98224302  0.66562357]]\n",
      "[ 1.95395221  8.62126376 -6.90929732  5.8511195 ]\n",
      "1\n",
      "[[-0.22550899 -0.69893516  0.44223119  0.4635144 ]\n",
      " [ 0.41924637 -0.84907101  0.44201946  0.81895096]\n",
      " [-0.31819419 -0.93157191 -0.88291063  0.96730709]]\n",
      "[ 0.22133531 -1.46202841  0.82839925  1.22574982]\n",
      "2\n",
      "[[-0.18535594 -0.28874483 -0.5699457 ]\n",
      " [ 0.73566822 -0.41395986  0.71734574]\n",
      " [ 0.92613781 -0.03210042  0.79083046]]\n",
      "[ 0.68013564 -0.26051684  0.36902275]\n",
      "[[-0.18535594 -0.28874483 -0.5699457 ]\n",
      " [ 0.73566822 -0.41395986  0.71734574]\n",
      " [ 0.92613781 -0.03210042  0.79083046]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.49243713, 0.15819807, 0.3493648 ]),\n",
       " {'Preactivation1': array([ 1.95395221,  8.62126376, -6.90929732,  5.8511195 ]),\n",
       "  'Activation1': array([8.75876951e-01, 9.99819800e-01, 9.97463216e-04, 9.97131575e-01]),\n",
       "  'Preactivation2': array([ 0.22133531, -1.46202841,  0.82839925,  1.22574982]),\n",
       "  'Activation2': array([0.55510903, 0.18815728, 0.69601635, 0.77307383]),\n",
       "  'Preactivation3': array([ 0.74471006, -0.39080887,  0.40145991]),\n",
       "  'Activation3': array([0.49243713, 0.15819807, 0.3493648 ])})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.feed_forward(np.array([4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  5.  6.  1.]\n",
      " [ 8.  5.  6.  1.]\n",
      " [12. 10.  3.  1.]]\n",
      "0\n",
      "[[ 0.15827445 -0.19490208 -0.58804668  0.0919204 ]\n",
      " [ 0.5864056   0.69929738  0.48443954  0.12202542]\n",
      " [-0.33655052  0.77142713 -0.98224302  0.66562357]]\n",
      "[[  1.28573636  14.07189622 -10.26358669   9.33136778]\n",
      " [  0.35789506  10.23624777 -10.34046594   7.72596479]\n",
      " [  3.45842873   5.34065317  -3.5683719    3.28054562]\n",
      " [  0.40812953   1.27582243  -1.08585016   0.87956939]]\n",
      "1\n",
      "[[-0.22550899 -0.69893516  0.44223119  0.4635144 ]\n",
      " [ 0.41924637 -0.84907101  0.44201946  0.81895096]\n",
      " [-0.31819419 -0.93157191 -0.88291063  0.96730709]]\n",
      "[[-0.23841328 -1.95040921 -0.24936865  1.7828928 ]\n",
      " [-0.12295391 -2.47510362  0.00553522  2.24512879]\n",
      " [-0.00872182 -0.02560315 -0.02418696  0.02657412]\n",
      " [-0.11308889 -2.44537719  0.03310669  2.21431049]]\n",
      "2\n",
      "[[-0.18535594 -0.28874483 -0.5699457 ]\n",
      " [ 0.73566822 -0.41395986  0.71734574]\n",
      " [ 0.92613781 -0.03210042  0.79083046]]\n",
      "[[ 0.72461657 -0.33749498  0.47917917]\n",
      " [ 0.49116685 -0.08392842  0.37507219]\n",
      " [ 0.7451392  -0.34987303  0.50067486]\n",
      " [ 0.97575965 -0.63775927  0.5614099 ]]\n",
      "[[-0.18535594 -0.28874483 -0.5699457 ]\n",
      " [ 0.73566822 -0.41395986  0.71734574]\n",
      " [ 0.92613781 -0.03210042  0.79083046]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.12367822, 0.03954931, 0.0871321 ],\n",
       "        [0.12781457, 0.03916568, 0.09211306],\n",
       "        [0.12388196, 0.03954074, 0.08706998],\n",
       "        [0.11836159, 0.04008424, 0.08160854]]),\n",
       " {'Preactivation1': array([[  1.28573636,  14.07189622, -10.26358669,   9.33136778],\n",
       "         [  0.35789506,  10.23624777, -10.34046594,   7.72596479],\n",
       "         [  3.45842873,   5.34065317,  -3.5683719 ,   3.28054562],\n",
       "         [  0.40812953,   1.27582243,  -1.08585016,   0.87956939]]),\n",
       "  'Activation1': array([[7.83424650e-01, 9.99999226e-01, 3.48791414e-05, 9.99911407e-01],\n",
       "         [5.88530791e-01, 9.99964154e-01, 3.22982265e-05, 9.99558974e-01],\n",
       "         [9.69481512e-01, 9.95230122e-01, 2.74282085e-02, 9.63755347e-01],\n",
       "         [6.00639290e-01, 7.81737822e-01, 2.52400525e-01, 7.06732980e-01]]),\n",
       "  'Preactivation2': array([[-0.23841328, -1.95040921, -0.24936865,  1.7828928 ],\n",
       "         [-0.12295391, -2.47510362,  0.00553522,  2.24512879],\n",
       "         [-0.00872182, -0.02560315, -0.02418696,  0.02657412],\n",
       "         [-0.11308889, -2.44537719,  0.03310669,  2.21431049]]),\n",
       "  'Activation2': array([[0.44067741, 0.12450875, 0.4379789 , 0.8560537 ],\n",
       "         [0.46930019, 0.07762204, 0.5013838 , 0.90422953],\n",
       "         [0.49781956, 0.49359956, 0.49395356, 0.50664314],\n",
       "         [0.47175787, 0.07977726, 0.50827592, 0.90152726]]),\n",
       "  'Preactivation3': array([[ 0.75342571, -0.38670923,  0.40316787],\n",
       "         [ 0.78632303, -0.39645657,  0.45875922],\n",
       "         [ 0.75507163, -0.38692602,  0.40245462],\n",
       "         [ 0.70948678, -0.37327438,  0.3376764 ]]),\n",
       "  'Activation3': array([[0.12367822, 0.03954931, 0.0871321 ],\n",
       "         [0.12781457, 0.03916568, 0.09211306],\n",
       "         [0.12388196, 0.03954074, 0.08706998],\n",
       "         [0.11836159, 0.04008424, 0.08160854]])})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.feed_forward(np.array([[4,5,6],[8,5,6],[12,10,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
